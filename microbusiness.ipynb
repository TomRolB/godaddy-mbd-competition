{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c840101c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-03-09T12:08:10.964903Z",
     "iopub.status.busy": "2023-03-09T12:08:10.964340Z",
     "iopub.status.idle": "2023-03-09T12:08:13.618060Z",
     "shell.execute_reply": "2023-03-09T12:08:13.616688Z"
    },
    "papermill": {
     "duration": 2.671001,
     "end_time": "2023-03-09T12:08:13.621322",
     "exception": false,
     "start_time": "2023-03-09T12:08:10.950321",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reading datasets\n",
    "Some notes have to be made here:\n",
    "* The \"revealed\" dataset contains an extra training set that was released at an advanced stage of the competition.\n",
    "* \"Orders\" is a dataset which was not provided by the competition host, but added by me. It includes data about the orders people were given to stay at home during covid, indicating how their degree of strictness."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "710d24159bbe081e"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13ce7125",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-09T12:08:13.644214Z",
     "iopub.status.busy": "2023-03-09T12:08:13.643706Z",
     "iopub.status.idle": "2023-03-09T12:08:25.498876Z",
     "shell.execute_reply": "2023-03-09T12:08:25.497582Z"
    },
    "papermill": {
     "duration": 11.86986,
     "end_time": "2023-03-09T12:08:25.501816",
     "exception": false,
     "start_time": "2023-03-09T12:08:13.631956",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../input/godaddy-microbusiness-density-forecasting/train.csv\")\n",
    "revealed = pd.read_csv(\"../input/godaddy-microbusiness-density-forecasting/revealed_test.csv\")\n",
    "train = pd.concat([train, revealed])\n",
    "test = pd.read_csv(\"../input/godaddy-microbusiness-density-forecasting/test.csv\")\n",
    "census = pd.read_csv(\"../input/godaddy-microbusiness-density-forecasting/census_starter.csv\", index_col=\"cfips\")\n",
    "coords = pd.read_csv(\"../input/usa-counties-coordinates/cfips_location.csv\", index_col=\"cfips\")\n",
    "orders = pd.read_csv(\"../input/stay-at-home-orders/U.S._State_and_Territorial_Stay-At-Home_Orders__March_15__2020___May_31__2021_by_County_by_Day.csv\").drop(\"Citations\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8ebfc2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-09T12:08:25.524951Z",
     "iopub.status.busy": "2023-03-09T12:08:25.524423Z",
     "iopub.status.idle": "2023-03-09T12:08:25.532736Z",
     "shell.execute_reply": "2023-03-09T12:08:25.531314Z"
    },
    "papermill": {
     "duration": 0.022777,
     "end_time": "2023-03-09T12:08:25.535213",
     "exception": false,
     "start_time": "2023-03-09T12:08:25.512436",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SMAPE implementation from user Giba\n",
    "# This is the metric used to measure a competitor's performance\n",
    "def smape(y_true, y_pred):\n",
    "    smap = np.zeros(y_true.shape[0])\n",
    "    \n",
    "    num = np.abs(y_true - y_pred)\n",
    "    dem = ((np.abs(y_true) + np.abs(y_pred)) / 2)\n",
    "    \n",
    "    pos_ind = (y_true!=0)|(y_pred!=0)\n",
    "    smap[pos_ind] = num[pos_ind] / dem[pos_ind]\n",
    "    \n",
    "    return 100 * np.nanmean(smap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f394d93",
   "metadata": {
    "papermill": {
     "duration": 0.009746,
     "end_time": "2023-03-09T12:08:25.602601",
     "exception": false,
     "start_time": "2023-03-09T12:08:25.592855",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Basic features\n",
    "We define time-based features, such as \"year\" or \"month\", plus some general ones, like the mean MBD across a particular state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61d122d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-09T12:08:25.626294Z",
     "iopub.status.busy": "2023-03-09T12:08:25.624928Z",
     "iopub.status.idle": "2023-03-09T12:32:17.684019Z",
     "shell.execute_reply": "2023-03-09T12:32:17.682532Z"
    },
    "papermill": {
     "duration": 1432.074675,
     "end_time": "2023-03-09T12:32:17.687676",
     "exception": false,
     "start_time": "2023-03-09T12:08:25.613001",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [05:57<00:00, 44.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [09:03<00:00, 45.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [07:19<00:00, 43.99s/it]\n"
     ]
    }
   ],
   "source": [
    "# Concat training and test sets to ease preprocessing\n",
    "train[\"is_test\"] = 0\n",
    "test[\"is_test\"] = 1\n",
    "\n",
    "df_all = pd.concat([train, test]).sort_values(['cfips','row_id']).reset_index(drop=True)\n",
    "\n",
    "# Define some features\n",
    "df_all[\"first_day_of_month\"] = pd.to_datetime(df_all[\"first_day_of_month\"])\n",
    "df_all[\"year\"] = df_all[\"first_day_of_month\"].dt.year\n",
    "df_all[\"month\"] = df_all[\"first_day_of_month\"].dt.month\n",
    "df_all[\"is_january\"] = (df_all[\"month\"] == 1)\n",
    "\n",
    "df_all.drop(df_all[(df_all[\"is_test\"] == 1) & (df_all[\"year\"] == 2022) & (df_all[\"month\"] > 10)].index, axis=0, inplace=True)\n",
    "\n",
    "df_all['county'] = df_all.groupby('cfips')['county'].ffill()\n",
    "df_all['state'] = df_all.groupby('cfips')['state'].ffill()\n",
    "# Month count\n",
    "df_all[\"dcount\"] = df_all.groupby(['cfips'])['row_id'].cumcount()\n",
    "\n",
    "df_all['county_i'] = (df_all['county'] + df_all['state']).factorize()[0]\n",
    "df_all['state_i'] = df_all['state'].factorize()[0]\n",
    "\n",
    "# We will set the target as the relative change in MBD. This could\n",
    "# be a simpler objective for the final model.\n",
    "\n",
    "# However, since having 0 as the MBD for the previous month would result in\n",
    "# an undefined value (division by zero), we first replace those values by 1.\n",
    "df_all['target'] = df_all.groupby('cfips')['microbusiness_density'].shift(1)\n",
    "df_all[\"target\"] = (df_all[\"target\"] != 0) * df_all[\"target\"] + (df_all[\"target\"] == 0)\n",
    "df_all['target'] = df_all['microbusiness_density'] / df_all['target'] - 1\n",
    "\n",
    "counties = df_all[\"cfips\"].unique()\n",
    "states = df_all[\"state\"].unique()\n",
    "dcounts = df_all[\"dcount\"].unique()\n",
    "\n",
    "# Add covid stay-at-home orders, by the level of strictness (how much\n",
    "# freedom people have to go out)\n",
    "orders[\"Date\"] = pd.to_datetime(orders[\"Date\"])\n",
    "#orders = orders.set_index(\"Date\")\n",
    "orders[\"year\"] = orders[\"Date\"].dt.year\n",
    "orders[\"month\"] = orders[\"Date\"].dt.month\n",
    "\n",
    "# Take the monthly mean of the level of strictness\n",
    "for year in orders[\"year\"].unique():\n",
    "    print(year)\n",
    "    yr_idx = df_all[\"year\"] == year\n",
    "    yr_idx_orders = orders[\"year\"] == year\n",
    "    for month in tqdm(orders.loc[orders[\"year\"] == year, \"month\"].unique()):\n",
    "        mn_idx = df_all[\"month\"] == month\n",
    "        mn_idx_orders = orders[\"month\"] == month\n",
    "        for county in counties:\n",
    "            df_all.loc[yr_idx & mn_idx & (df_all[\"cfips\"] == county), \"order_level\"] = 4 - orders.loc[yr_idx_orders & mn_idx_orders & (orders[\"FIPS_Code\"] == county), \"SAH_Order_Code\"].mean()\n",
    "\n",
    "\n",
    "for county in counties:\n",
    "    # Add the county's coordinates\n",
    "    df_all.loc[df_all[\"cfips\"] == county, \"lng\"] = coords.loc[county, \"lng\"]\n",
    "    df_all.loc[df_all[\"cfips\"] == county, \"lat\"] = coords.loc[county, \"lat\"]\n",
    "    # Compute a rolling mean of the level of strictness\n",
    "    df_all.loc[df_all[\"cfips\"] == county, \"mean_level\"] = df_all[\"order_level\"].rolling(6).mean()\n",
    "    df_all.loc[(df_all[\"cfips\"] == county) & df_all[\"mean_level\"].isnull(), \"mean_level\"] = 0\n",
    "\n",
    "for state in states:\n",
    "    for dc in dcounts:\n",
    "        # Take the target's monthly mean and std for each state\n",
    "        state_idx = (df_all[\"dcount\"] == dc) & (df_all[\"state\"] == state)\n",
    "        s = df_all.loc[state_idx, \"target\"]\n",
    "        df_all.loc[state_idx, \"mean_state_tg\"] = s.mean()\n",
    "        df_all.loc[state_idx, \"state_tg_std\"] = s.std()\n",
    "        \n",
    "# District of Columbia has a single county, so std cannot be computed\n",
    "df_all.loc[df_all[\"state\"] == \"District of Columbia\", \"state_tg_std\"] = 0\n",
    "\n",
    "# Split sets again\n",
    "train = df_all.loc[df_all[\"is_test\"] == 0].drop(\"is_test\", axis=1)\n",
    "test = df_all.loc[df_all[\"is_test\"] == 1].drop(\"is_test\", axis=1)\n",
    "\n",
    "# Compute marginal target and marginal MBD (the true difference between\n",
    "# a month's value and that from the previous month)\n",
    "for county in counties:\n",
    "    county_df = train[train[\"cfips\"] == county]\n",
    "    \n",
    "    train.loc[train[\"cfips\"] == county, \"prev_marg_density\"] = county_df[\"microbusiness_density\"].shift(1) - county_df[\"microbusiness_density\"].shift(2)\n",
    "    train.loc[train[\"cfips\"] == county, \"prev_marg_target\"] = county_df[\"target\"].shift(1) - county_df[\"target\"].shift(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Linear models and lags\n",
    "We will fit a linear and a lasso regression for each count. Their predictions and their slopes will be used as extra features.\n",
    "\n",
    "On the other hand, we will create MBD lags (features that reflect values from previous months) and moving averages."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c3f7d082aef157ca"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87996204",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-09T12:32:19.771858Z",
     "iopub.status.busy": "2023-03-09T12:32:19.771358Z",
     "iopub.status.idle": "2023-03-09T12:32:19.835448Z",
     "shell.execute_reply": "2023-03-09T12:32:19.834178Z"
    },
    "papermill": {
     "duration": 0.086,
     "end_time": "2023-03-09T12:32:19.838493",
     "exception": false,
     "start_time": "2023-03-09T12:32:19.752493",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_names = [\"LR\", \n",
    "    #\"ridge\", \n",
    "    \"lasso\", \n",
    "    #\"elasticnet\"\n",
    "]\n",
    "\n",
    "sorted_train = train.sort_values(\"first_day_of_month\")\n",
    "sorted_test = test.sort_values(\"first_day_of_month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d41114c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-09T12:32:19.922294Z",
     "iopub.status.busy": "2023-03-09T12:32:19.921437Z",
     "iopub.status.idle": "2023-03-09T12:32:19.930895Z",
     "shell.execute_reply": "2023-03-09T12:32:19.929817Z"
    },
    "papermill": {
     "duration": 0.030959,
     "end_time": "2023-03-09T12:32:19.933557",
     "exception": false,
     "start_time": "2023-03-09T12:32:19.902598",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Using TimeSeriesSplit creates a split where an uneven number of rows\n",
    "# per county is assigned to the indexes. Therefore, we have to create\n",
    "# a custom time split.\n",
    "def custom_split(df, n_splits):\n",
    "    # custom_split assumes df is sorted by first_day_of_month\n",
    "    county_size = df.groupby(\"cfips\").size()\n",
    "    if not (county_size.min() == county_size.max()):\n",
    "        print(\"WARNING: Dataframe does not have an equal number of rows per county\")\n",
    "    n_months = len(df[\"first_day_of_month\"].unique())\n",
    "    split_len = int(n_months / (n_splits + 1)) * len(counties)\n",
    "    \n",
    "    last = split_len\n",
    "    splits = []\n",
    "    for i in range(1, n_splits + 1):\n",
    "        if i == n_splits:\n",
    "            splits.append((np.arange(0, last), np.arange(last, df.shape[0])))\n",
    "            break\n",
    "        splits.append((np.arange(0, last), np.arange(last, last + split_len)))\n",
    "        last += split_len\n",
    "        \n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861d7a5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-09T12:32:20.005222Z",
     "iopub.status.busy": "2023-03-09T12:32:20.004240Z",
     "iopub.status.idle": "2023-03-09T12:33:35.598976Z",
     "shell.execute_reply": "2023-03-09T12:33:35.597686Z"
    },
    "papermill": {
     "duration": 75.63437,
     "end_time": "2023-03-09T12:33:35.618752",
     "exception": false,
     "start_time": "2023-03-09T12:32:19.984382",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_index, valid_index = custom_split(sorted_train, 8)[-1]\n",
    "nr_feats = list(sorted_train.columns)\n",
    "\n",
    "st_dcount = sorted(sorted_train[\"dcount\"].unique())\n",
    "        \n",
    "sub_train = sorted_train.iloc[train_index]\n",
    "valid = sorted_train.iloc[valid_index]\n",
    "\n",
    "cond_1 = (sub_train.groupby(\"cfips\").size().min() == sub_train.groupby(\"cfips\").size().max())\n",
    "cond_2 = (valid.groupby(\"cfips\").size().min() == valid.groupby(\"cfips\").size().max())\n",
    "\n",
    "if not (cond_1 and cond_2):\n",
    "    print(\"WARNING: Subtraining and/or validation set do not have an equal number of rows per county\")\n",
    "\n",
    "# Initialize columns to nan\n",
    "for name in model_names:\n",
    "    sub_train.loc[:, name] = np.nan\n",
    "    valid.loc[:, name] = np.nan\n",
    "    sorted_test.loc[:, name] = np.nan\n",
    "    \n",
    "    nr_feats.append(name)\n",
    "    nr_feats.append(name + \"_slope\")\n",
    "\n",
    "# Anomaly detection (dismissed; read note below) and linear features\n",
    "anom_rates = []\n",
    "for county in counties:\n",
    "    county_train_df = sub_train[sub_train[\"cfips\"] == county]\n",
    "    county_valid_df = valid[valid[\"cfips\"] == county]\n",
    "    ctd_shape = county_train_df.shape[0]\n",
    "    cvd_shape = county_valid_df.shape[0]\n",
    "    cttd_shape = test[test[\"cfips\"] == county].shape[0]\n",
    "\n",
    "    county_X_train = np.arange(ctd_shape).reshape(-1, 1)\n",
    "    county_X_valid = np.arange(ctd_shape, ctd_shape + cvd_shape).reshape(-1, 1)\n",
    "    county_X_test = np.arange(ctd_shape + cvd_shape, ctd_shape + cvd_shape + cttd_shape).reshape(-1, 1)\n",
    "\n",
    "    # Detect anomalies in time series and replace them by the expected\n",
    "    # value for the time step in question, i.e. trend + seasonal\n",
    "    \n",
    "    # NOTE: This piece of code has no effect over the final dataset. It\n",
    "    # was used to test a preprocessing step, but was then dismissed.\n",
    "    county_y_train = county_train_df[\"microbusiness_density\"]\n",
    "    if county_y_train.shape[0] >= 24:\n",
    "        county_y_train.index = county_train_df[\"first_day_of_month\"]\n",
    "        sd = seasonal_decompose(county_y_train, model=\"additive\", \n",
    "                                extrapolate_trend=\"freq\")\n",
    "        q1, q3 = np.percentile(county_y_train.sort_values(),[25,75])\n",
    "        iqr = q3 - q1\n",
    "        low = q1 - (1.5 * iqr)\n",
    "        up = q3 + (1.5 * iqr) \n",
    "        anom = (county_y_train < low) | (county_y_train > up)\n",
    "        # Multiply by boolean array, so that only anomalies change\n",
    "        county_y_train = (1 - anom) * county_y_train + anom * (sd.seasonal + sd.trend)\n",
    "        anom_rates.append(anom.sum() / anom.shape[0])\n",
    "\n",
    "    # Fit a linear (standard and lasso) regression over the MBD county-wise.\n",
    "    # We will feed the final model with the predictions from these regressions\n",
    "    # and their slopes.\n",
    "    models = [LinearRegression(), \n",
    "              #Ridge(), \n",
    "              Lasso(), \n",
    "              #ElasticNet(alpha=0.3)\n",
    "    ]\n",
    "    \n",
    "    for model, name in zip(models, model_names):\n",
    "        model.fit(county_X_train, county_y_train)\n",
    "\n",
    "        train_preds = model.predict(county_X_train)\n",
    "        valid_preds = model.predict(county_X_valid)\n",
    "        test_preds = model.predict(county_X_test)\n",
    "        \n",
    "        # Add linear predictions\n",
    "        sub_train.loc[sub_train[\"cfips\"] == county, name] = train_preds\n",
    "        valid.loc[valid[\"cfips\"] == county, name] = valid_preds\n",
    "        sorted_test.loc[sorted_test[\"cfips\"] == county, name] = test_preds\n",
    "        \n",
    "        # Add slopes\n",
    "        sub_train.loc[sub_train[\"cfips\"] == county, name + \"_slope\"] = model.coef_[0]\n",
    "        valid.loc[valid[\"cfips\"] == county, name + \"_slope\"] = model.coef_[0]\n",
    "        sorted_test.loc[sorted_test[\"cfips\"] == county, name + \"_slope\"] = model.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f23a8e06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-09T12:33:35.657374Z",
     "iopub.status.busy": "2023-03-09T12:33:35.656053Z",
     "iopub.status.idle": "2023-03-09T12:39:12.884795Z",
     "shell.execute_reply": "2023-03-09T12:39:12.882807Z"
    },
    "papermill": {
     "duration": 337.251843,
     "end_time": "2023-03-09T12:39:12.888007",
     "exception": false,
     "start_time": "2023-03-09T12:33:35.636164",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3135/3135 [04:10<00:00, 12.53it/s]\n",
      "100%|██████████| 3135/3135 [01:24<00:00, 36.98it/s]\n"
     ]
    }
   ],
   "source": [
    "# Non-recurrent features\n",
    "sub_train[\"set\"] = \"train\"\n",
    "valid[\"set\"] = \"valid\"\n",
    "sorted_test[\"set\"] = \"test\"\n",
    "nr_all = pd.concat([sub_train, valid, sorted_test])[nr_feats + [\"set\"]]\n",
    "\n",
    "# Create time features. This includes lags (MBD and target from the previous\n",
    "# months) and moving averages with their std.\n",
    "for county in tqdm(counties):\n",
    "    # Add lag 6 separately so we do not compute mean and std for a window\n",
    "    # of 1 (which would not have sense)\n",
    "    nr_all.loc[nr_all[\"cfips\"] == county, \"mbd_lag_5\"] = nr_all.loc[nr_all[\"cfips\"] == county, \"microbusiness_density\"].shift(5)\n",
    "    nr_all.loc[nr_all[\"cfips\"] == county, \"tg_lag_5\"] = nr_all.loc[nr_all[\"cfips\"] == county, \"target\"].shift(5)\n",
    "    nr_all.loc[nr_all[\"cfips\"] == county, \"mean_state_tg_lag_5\"] = nr_all.loc[nr_all[\"cfips\"] == county, \"mean_state_tg\"].shift(5)\n",
    "    nr_all.loc[nr_all[\"cfips\"] == county, \"state_tg_std_lag_5\"] = nr_all.loc[nr_all[\"cfips\"] == county, \"state_tg_std\"].shift(5)\n",
    "    for j in range(2, 5):\n",
    "        # Add MBD from j+4 months ago\n",
    "        nr_all.loc[nr_all[\"cfips\"] == county, \"mbd_lag_\" + str(j+4)] = nr_all.loc[nr_all[\"cfips\"] == county, \"microbusiness_density\"].shift(j+4)\n",
    "        # Add target from j+4 months ago\n",
    "        nr_all.loc[nr_all[\"cfips\"] == county, \"tg_lag_\" + str(j+4)] = nr_all.loc[nr_all[\"cfips\"] == county, \"target\"].shift(j+4)\n",
    "        # Add the state's target mean from j+4 months ago\n",
    "        nr_all.loc[nr_all[\"cfips\"] == county, \"mean_state_tg_lag_\" + str(j+4)] = nr_all.loc[nr_all[\"cfips\"] == county, \"mean_state_tg\"].shift(j+4)\n",
    "        # Add the state's target std from j+4 months ago\n",
    "        nr_all.loc[nr_all[\"cfips\"] == county, \"state_tg_std_lag_\" + str(j+4)] = nr_all.loc[nr_all[\"cfips\"] == county, \"state_tg_std\"].shift(j+4)\n",
    "        \n",
    "        # Compute the MBD's rolling mean and std (window starts 5+j months ago \n",
    "        # and ends 5 months ago)\n",
    "        s = nr_all.loc[nr_all[\"cfips\"] == county, \"mbd_lag_5\"].rolling(window=j)\n",
    "        nr_all.loc[nr_all[\"cfips\"] == county, \"MA_\" + str(j)] = s.mean().shift(1)\n",
    "        nr_all.loc[nr_all[\"cfips\"] == county, \"Mstd_\" + str(j)] = s.std().shift(1)\n",
    "\n",
    "        # Compute the target's rolling mean and std (same window)\n",
    "        t = nr_all.loc[nr_all[\"cfips\"] == county, \"tg_lag_5\"].rolling(window=j)\n",
    "        nr_all.loc[nr_all[\"cfips\"] == county, \"t_MA_\" + str(j)] = t.mean().shift(1)\n",
    "        nr_all.loc[nr_all[\"cfips\"] == county, \"t_Mstd_\" + str(j)] = t.std().shift(1)\n",
    "\n",
    "        # Compute the rolling mean and std (same window) for the overall state's \n",
    "        # monthly target mean\n",
    "        u = nr_all.loc[nr_all[\"cfips\"] == county, \"mean_state_tg_lag_5\"].rolling(window=j)\n",
    "        nr_all.loc[nr_all[\"cfips\"] == county, \"state_t_MA_\" + str(j)] = u.mean().shift(1)\n",
    "        nr_all.loc[nr_all[\"cfips\"] == county, \"state_t_Mstd_\" + str(j)] = u.std().shift(1)\n",
    "\n",
    "# Group counties in clusters\n",
    "# NOTE: The resulting features did not have an important effect in the\n",
    "# final model's performance. Instead, lag features and \"linear\" ones\n",
    "# were the key.\n",
    "\n",
    "cluster_df = nr_all[[\"lng\", \"lat\", \"mbd_lag_5\"]].copy()\n",
    "\n",
    "# Scale features before clustering\n",
    "lat_max = cluster_df[\"lat\"].max()\n",
    "idx = (nr_all[\"dcount\"] == 6)\n",
    "cluster_df[\"lat\"] += abs(cluster_df[\"lat\"].min())\n",
    "cluster_df[\"lat\"] /= lat_max\n",
    "cluster_df[\"lng\"] += abs(cluster_df[\"lng\"].min())\n",
    "cluster_df[\"lng\"] /= lat_max\n",
    "cluster_df[\"mbd_lag_5\"] /= (cluster_df[\"mbd_lag_5\"].max() * 2)\n",
    "\n",
    "kmeans = KMeans(20, random_state=22) \n",
    "kmeans.fit(cluster_df.loc[idx])\n",
    "\n",
    "nan_index = np.logical_not(cluster_df.isnull().any(axis=1))\n",
    "\n",
    "nr_all.loc[nan_index, \"cluster_lag_5\"] = kmeans.predict(cluster_df.loc[nan_index])\n",
    "\n",
    "for county in tqdm(counties):\n",
    "    for j in range(2, 5):\n",
    "        # Compute lags and rolling mean and std for the cluster label\n",
    "        # each county was given\n",
    "        \n",
    "        nr_all.loc[(nr_all[\"cfips\"] == county) & nan_index, \"cluster_lag_\" + str(j+4)] = nr_all.loc[(nr_all[\"cfips\"] == county) & nan_index, \"cluster_lag_5\"].shift(j-1)\n",
    "        \n",
    "        s = nr_all.loc[(nr_all[\"cfips\"] == county) & nan_index, \"cluster_lag_5\"].rolling(j)\n",
    "        nr_all.loc[(nr_all[\"cfips\"] == county) & nan_index, \"cluster_MA_\" + str(j)] = s.mean().shift(1)\n",
    "        nr_all.loc[(nr_all[\"cfips\"] == county) & nan_index, \"cluster_Mstd_\" + str(j)] = s.std().shift(1)\n",
    "    \n",
    "# Separate data into the three different original sets\n",
    "nr_sub_train = nr_all[nr_all[\"set\"] == \"train\"].drop(\"set\", axis=1).copy()\n",
    "nr_valid = nr_all[nr_all[\"set\"] == \"valid\"].drop(\"set\", axis=1).copy()\n",
    "nr_test = nr_all[nr_all[\"set\"] == \"test\"].drop(\"set\", axis=1).copy()\n",
    "\n",
    "sub_train = sub_train.drop(\"set\", axis=1)\n",
    "valid = valid.drop(\"set\", axis=1)\n",
    "sorted_test = sorted_test.drop(\"set\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Final model and submission\n",
    "Fit a CatBoostRegressor to the features we defined. Then, convert the target back to its valid form and prepare data for submission."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "756a8dc4ed4e8582"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af5d1f7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-09T12:39:13.547901Z",
     "iopub.status.busy": "2023-03-09T12:39:13.547368Z",
     "iopub.status.idle": "2023-03-09T12:41:15.644504Z",
     "shell.execute_reply": "2023-03-09T12:41:15.643127Z"
    },
    "papermill": {
     "duration": 122.265898,
     "end_time": "2023-03-09T12:41:15.648077",
     "exception": false,
     "start_time": "2023-03-09T12:39:13.382179",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nr_final_all = pd.concat([nr_sub_train.dropna(), nr_valid])\n",
    "nr_final_all = nr_final_all.sort_values(\"first_day_of_month\")\n",
    "nr_final_all = nr_final_all.drop(columns=[\"row_id\", \"county\", \"state\", \"active\"])\n",
    "\n",
    "nr_final_test = nr_test.sort_values(\"first_day_of_month\")\n",
    "nr_final_test_row_id = nr_final_test[\"row_id\"]\n",
    "nr_final_test = nr_final_test.drop(columns=[\"row_id\", \"county\", \"state\", \"active\"])\n",
    "\n",
    "nr_preds = pd.DataFrame()\n",
    "\n",
    "# Define the final model\n",
    "nr_cat = CatBoostRegressor(loss_function=\"MAPE\",\n",
    "                           iterations=800,\n",
    "                           grow_policy='SymmetricTree',\n",
    "                           verbose=0,\n",
    "                           learning_rate=0.035,\n",
    "                           max_depth=6,\n",
    "                           l2_leaf_reg=0.2,\n",
    "                           subsample=0.50,\n",
    "                           max_bin=4096)\n",
    "\n",
    "# Train the model. We first drop the columns which introduce target leakage or which\n",
    "# do not help the model at all.\n",
    "nr_cat.fit(X=nr_final_all.drop(columns=[\"target\", \"cfips\", \"microbusiness_density\", \"mean_state_tg\", \"state_tg_std\", \"first_day_of_month\"]),\n",
    "           y=nr_final_all[\"target\"],\n",
    "           verbose=0)\n",
    "# Compute predictions for the test set\n",
    "nr_final_test[\"target_preds\"] = nr_cat.predict(nr_final_test.drop(columns=[\"target\", \"cfips\", \"microbusiness_density\", \"mean_state_tg\", \"state_tg_std\", \"first_day_of_month\"]))\n",
    "\n",
    "test_dc = sorted(nr_final_test[\"dcount\"].unique())\n",
    "\n",
    "# Convert \"target\" to microbusiness density (remember: \"target\" was the relative\n",
    "# change in MBD, and we want the MBD per se)\n",
    "\n",
    "# Note we first perform this operation separately for the first month of the \n",
    "# test set. This is because we need the previous month's microbusiness density -\n",
    "# which is precisely the last month of the training set - to convert the target.\n",
    "\n",
    "# First month:\n",
    "for county in counties:\n",
    "    idx = (nr_final_test[\"cfips\"] == county) & (nr_final_test[\"dcount\"] == test_dc[0])\n",
    "    nr_final_test.loc[idx , \"microbusiness_density\"] = (nr_final_test.loc[idx, \"target_preds\"] + 1) * nr_final_all.loc[(nr_final_all[\"cfips\"] == county) & (nr_final_all[\"dcount\"] == (test_dc[0] - 1)), \"microbusiness_density\"].values[0]\n",
    "\n",
    "# Rest of the months:\n",
    "for dc in test_dc[1:]:\n",
    "    for county in counties:\n",
    "        idx = (nr_final_test[\"cfips\"] == county) & (nr_final_test[\"dcount\"] == dc)\n",
    "        nr_final_test.loc[idx , \"microbusiness_density\"] = (nr_final_test.loc[idx, \"target_preds\"] + 1) * nr_final_test.loc[(nr_final_test[\"cfips\"] == county) & (nr_final_test[\"dcount\"] == (dc - 1)), \"microbusiness_density\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "25de476d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-09T12:41:19.340325Z",
     "iopub.status.busy": "2023-03-09T12:41:19.339430Z",
     "iopub.status.idle": "2023-03-09T12:41:19.433632Z",
     "shell.execute_reply": "2023-03-09T12:41:19.432256Z"
    },
    "papermill": {
     "duration": 0.257319,
     "end_time": "2023-03-09T12:41:19.436676",
     "exception": false,
     "start_time": "2023-03-09T12:41:19.179357",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>microbusiness_density</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001_2022-11-01</td>\n",
       "      <td>3.442677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001_2022-12-01</td>\n",
       "      <td>3.470915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1003_2022-11-01</td>\n",
       "      <td>8.257636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1003_2022-12-01</td>\n",
       "      <td>8.250630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1005_2022-11-01</td>\n",
       "      <td>1.247223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25075</th>\n",
       "      <td>1117_2023-06-01</td>\n",
       "      <td>6.664164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25076</th>\n",
       "      <td>55031_2023-06-01</td>\n",
       "      <td>3.507871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25077</th>\n",
       "      <td>17113_2023-06-01</td>\n",
       "      <td>4.799820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25078</th>\n",
       "      <td>29045_2023-06-01</td>\n",
       "      <td>0.692387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25079</th>\n",
       "      <td>56045_2023-06-01</td>\n",
       "      <td>1.497785</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25080 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 row_id  microbusiness_density\n",
       "0       1001_2022-11-01               3.442677\n",
       "1       1001_2022-12-01               3.470915\n",
       "2       1003_2022-11-01               8.257636\n",
       "3       1003_2022-12-01               8.250630\n",
       "4       1005_2022-11-01               1.247223\n",
       "...                 ...                    ...\n",
       "25075   1117_2023-06-01               6.664164\n",
       "25076  55031_2023-06-01               3.507871\n",
       "25077  17113_2023-06-01               4.799820\n",
       "25078  29045_2023-06-01               0.692387\n",
       "25079  56045_2023-06-01               1.497785\n",
       "\n",
       "[25080 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the predictions as a csv to submit them\n",
    "\n",
    "preds_2022 = pd.concat([revealed[\"row_id\"].reset_index(drop=True), revealed[\"microbusiness_density\"].reset_index(drop=True)], axis=1)\n",
    "preds_2023 = pd.concat([nr_final_test_row_id.reset_index(drop=True), nr_final_test[\"microbusiness_density\"].reset_index(drop=True)], axis=1)\n",
    "preds_2023.index = preds_2023.index + preds_2022.shape[0]\n",
    "\n",
    "subm = pd.concat([preds_2022, preds_2023])\n",
    "subm.to_csv('submission.csv', index=False)\n",
    "subm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2003.234861,
   "end_time": "2023-03-09T12:41:23.270603",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-03-09T12:08:00.035742",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
